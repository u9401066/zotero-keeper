"""
Batch Import Tools for Zotero Keeper

High-performance batch import with complete metadata preservation.
Uses direct Python import from pubmed-search library (submodule).

Architecture:
- zotero-keeper directly imports pubmed-search as Python library
- Data flows: pubmed-search â†’ mapper â†’ Zotero
- No Agent intermediary - complete metadata preserved!

Tools:
- batch_import_from_pubmed: Primary batch import (PMID â†’ complete metadata â†’ Zotero)
"""

import logging
import time
from typing import Any

logger = logging.getLogger(__name__)

# Import domain entities  # noqa: E402
from ...domain.entities.batch_result import (
    BatchImportResult,
    ImportAction,
    ImportedItem,
)

# Import mappers  # noqa: E402
from ..mappers.pubmed_mapper import (
    map_pubmed_to_zotero,
)

# Import pubmed integration  # noqa: E402
from ..pubmed import fetch_pubmed_articles, get_pubmed_client, is_using_submodule

# Check if pubmed-search is available
try:
    _pubmed_client = get_pubmed_client()
    BATCH_IMPORT_AVAILABLE = True
    if is_using_submodule():
        logger.info("Batch import enabled (using submodule)")
    else:
        logger.info("Batch import enabled (using installed package)")
except ImportError as e:
    BATCH_IMPORT_AVAILABLE = False
    logger.warning(
        f"pubmed-search not available: {e}\n"
        "Batch import disabled. Options:\n"
        "  1. Development: git submodule update --init --recursive\n"
        "  2. Production: uv pip install pubmed-search-mcp"
    )


def is_batch_import_available() -> bool:
    """Check if batch import is available."""
    return BATCH_IMPORT_AVAILABLE


def register_batch_tools(mcp, zotero_client):
    """
    Register batch import tools.

    Args:
        mcp: FastMCP server instance
        zotero_client: ZoteroClient instance
    """

    @mcp.tool()
    async def batch_import_from_pubmed(
        pmids: str,
        tags: list[str] | None = None,
        skip_duplicates: bool = True,
        collection_key: str | None = None,
        collection_name: str | None = None,
        include_citation_metrics: bool = True,
    ) -> dict[str, Any]:
        """
        ðŸ“¦ Batch import PubMed articles to Zotero with complete metadata

        æ‰¹æ¬¡åŒ¯å…¥ PubMed æ–‡ç»åˆ° Zoteroï¼Œä¿ç•™å®Œæ•´çš„ metadataï¼

        Features:
        - âœ… Complete abstract (not truncated!)
        - âœ… Author keywords + MeSH terms â†’ tags
        - âœ… PMID, PMCID, affiliations â†’ extra field
        - âœ… Batch duplicate detection (by PMID/DOI)
        - âœ… Detailed result reporting
        - âœ… Collection validation (é˜²å‘†æ©Ÿåˆ¶!)
        - âœ… Citation metrics (RCR, percentile) â†’ extra field (default ON!)

        âš ï¸ IMPORTANT - é˜²å‘†æé†’:
        - ä½¿ç”¨ collection_name åƒæ•¸ (æŽ¨è–¦!) å¯è‡ªå‹•é©—è­‰åç¨±æ˜¯å¦å­˜åœ¨
        - å¦‚æžœåç¨±ä¸å­˜åœ¨ï¼Œæœƒå›žå‚³å¯ç”¨çš„ collections æ¸…å–®
        - é¿å…ä½¿ç”¨ collection_keyï¼Œé™¤éžä½ ç¢ºå®š key æ˜¯å°çš„
        - éœ€è¦æŸ¥çœ‹ collectionsï¼Ÿå…ˆå‘¼å« list_collections() æˆ– get_collection_tree()

        Args:
            pmids: Comma-separated PMIDs (e.g., "38353755,37864754")
                   or "last" to use results from last search (requires session)
            tags: Additional tags to apply to all imported articles
            skip_duplicates: Skip if exact PMID or DOI match found (default: True)
            collection_key: Zotero collection key (âš ï¸ ä¸å»ºè­°ç›´æŽ¥ä½¿ç”¨ï¼Œå®¹æ˜“å‡ºéŒ¯)
            collection_name: Collection name (æŽ¨è–¦! è‡ªå‹•é©—è­‰ä¸¦è§£æžç‚º key)
            include_citation_metrics: If True (default), fetch RCR/percentile from iCite
                                      and add to extra field

        Returns:
            Detailed import result:
            {
                "success": true,
                "total": 30,
                "added": 27,
                "skipped": 2 (duplicates),
                "warnings": 1 (possible duplicates),
                "failed": 0,
                "added_items": [...],
                "skipped_items": [...],
                "elapsed_time": 12.5,
                "collection_info": {"key": "ABC123", "name": "test1"}  // ç¢ºèªç”¨!
            }

        Example:
            # âœ… æŽ¨è–¦ï¼šä½¿ç”¨ collection_name (æœƒè‡ªå‹•é©—è­‰!)
            # RCR é è¨­æœƒè‡ªå‹•å–å¾—ä¸¦å­˜å…¥ extra æ¬„ä½
            batch_import_from_pubmed(
                pmids="38353755,37864754",
                collection_name="test1"
            )

            # å¦‚æžœä¸éœ€è¦ RCR (è¼ƒå¿«)
            batch_import_from_pubmed(
                pmids="38353755,37864754",
                collection_name="test1",
                include_citation_metrics=False
            )

        Workflow:
            1. (å¯é¸) zotero-keeper: list_collections() â†’ ç¢ºèªç›®æ¨™ collection åç¨±
            2. pubmed-search: search_literature("AI anesthesia") â†’ PMIDs
            3. zotero-keeper: batch_import_from_pubmed(pmids, collection_name="xxx")
        """
        if not BATCH_IMPORT_AVAILABLE:
            return {
                "success": False,
                "error": "pubmed-search submodule not available",
                "hint": "Run: git submodule update --init --recursive",
            }

        start_time = time.time()
        result = BatchImportResult()

        # === é˜²å‘†æ©Ÿåˆ¶ 1: Collection é©—è­‰ ===
        validated_collection_key = None
        collection_info = None

        if collection_name or collection_key:
            try:
                if collection_name and not collection_key:
                    # ç”¨åç¨±æŸ¥æ‰¾ collection
                    collections = await zotero_client.list_collections()
                    found = None
                    for col in collections:
                        if col.get("name", "").lower() == collection_name.lower():
                            found = col
                            break

                    if not found:
                        # æä¾›ç›¸ä¼¼åç¨±å»ºè­°
                        similar = [c.get("name") for c in collections if collection_name.lower() in c.get("name", "").lower()][:5]
                        return {
                            "success": False,
                            "error": f"Collection '{collection_name}' not found",
                            "hint": f"Similar collections: {similar}" if similar else "Use list_collections() to see available collections",
                            "available_collections": [{"key": c.get("key"), "name": c.get("name")} for c in collections[:10]],
                        }

                    validated_collection_key = found.get("key")
                    collection_info = {
                        "key": validated_collection_key,
                        "name": found.get("name"),
                        "resolved_from": "name",
                    }
                    logger.info(f"Resolved collection '{collection_name}' â†’ key: {validated_collection_key}")

                elif collection_key:
                    # é©—è­‰ collection_key æ˜¯å¦å­˜åœ¨
                    collections = await zotero_client.list_collections()
                    found = None
                    for col in collections:
                        if col.get("key") == collection_key:
                            found = col
                            break

                    if not found:
                        return {
                            "success": False,
                            "error": f"Collection key '{collection_key}' not found",
                            "hint": "Use list_collections() to see available collections",
                            "available_collections": [{"key": c.get("key"), "name": c.get("name")} for c in collections[:10]],
                        }

                    validated_collection_key = collection_key
                    collection_info = {
                        "key": validated_collection_key,
                        "name": found.get("name"),
                        "resolved_from": "key",
                    }
                    logger.info(f"Validated collection key: {validated_collection_key} ({found.get('name')})")

            except Exception as e:
                logger.warning(f"Collection validation failed: {e}")
                # å¦‚æžœé©—è­‰å¤±æ•—ä½†æœ‰ keyï¼Œç¹¼çºŒä½¿ç”¨ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
                if collection_key:
                    validated_collection_key = collection_key
                    collection_info = {
                        "key": collection_key,
                        "name": "unknown (validation failed)",
                        "warning": str(e),
                    }

        try:
            # 1. Parse PMIDs
            pmid_list = _parse_pmids(pmids)

            if not pmid_list:
                return {
                    "success": False,
                    "error": "No valid PMIDs provided",
                    "hint": "Provide comma-separated PMIDs, e.g., '38353755,37864754'",
                }

            logger.info(f"Batch import: {len(pmid_list)} PMIDs")

            # 2. Fetch complete metadata from pubmed-search library
            # This is DIRECT Python import, not MCP call!
            # Data is complete and not truncated!
            try:
                articles = fetch_pubmed_articles(pmid_list)
            except Exception as e:
                logger.error(f"Failed to fetch from PubMed: {e}")
                return {
                    "success": False,
                    "error": f"Failed to fetch article details: {e}",
                }

            if not articles:
                return {
                    "success": False,
                    "error": "No articles found for provided PMIDs",
                    "pmids_provided": len(pmid_list),
                }

            logger.info(f"Fetched {len(articles)} articles from PubMed")

            # 2.5. Fetch citation metrics if requested
            citation_metrics = {}
            if include_citation_metrics:
                try:
                    from ..pubmed import get_pubmed_client

                    client = get_pubmed_client()
                    # Import LiteratureSearcher from pubmed_search
                    # Note: pubmed_search is configured via ../pubmed/__init__.py
                    from pubmed_search import LiteratureSearcher  # type: ignore

                    searcher = LiteratureSearcher(
                        email=getattr(client, "email", "zotero@example.com"), api_key=getattr(client, "api_key", None)
                    )
                    citation_metrics = searcher.get_citation_metrics(pmid_list)
                    logger.info(f"Fetched citation metrics for {len(citation_metrics)} articles")
                except ImportError as e:
                    logger.warning(f"Cannot import LiteratureSearcher for citation metrics: {e}")
                except Exception as e:
                    logger.warning(f"Failed to fetch citation metrics: {e}")
                    # Continue without metrics

            # Merge citation metrics into articles
            if citation_metrics:
                for article in articles:
                    pmid = article.get("pmid", "")
                    if pmid in citation_metrics:
                        metrics = citation_metrics[pmid]
                        # Add metrics fields to article
                        article["relative_citation_ratio"] = metrics.get("relative_citation_ratio")
                        article["nih_percentile"] = metrics.get("nih_percentile")
                        article["citation_count"] = metrics.get("citation_count")
                        article["citations_per_year"] = metrics.get("citations_per_year")
                        article["apt"] = metrics.get("apt")

            # 3. Check for duplicates (batch)
            existing_identifiers = {"existing_pmids": set(), "existing_dois": set()}
            pmid_to_key = {}
            doi_to_key = {}

            if skip_duplicates:
                # Extract DOIs from articles
                article_dois = [a.get("doi", "").lower() for a in articles if a.get("doi")]

                try:
                    check_result = await zotero_client.batch_check_identifiers(
                        pmids=pmid_list,
                        dois=article_dois,
                    )
                    existing_identifiers = check_result
                    pmid_to_key = check_result.get("pmid_to_key", {})
                    doi_to_key = check_result.get("doi_to_key", {})

                    logger.info(
                        f"Duplicate check: {len(existing_identifiers['existing_pmids'])} PMIDs, "
                        f"{len(existing_identifiers['existing_dois'])} DOIs already exist"
                    )
                except Exception as e:
                    logger.warning(f"Duplicate check failed: {e}")
                    # Continue without duplicate checking

            # 4. Process each article
            items_to_save = []

            for article in articles:
                pmid = article.get("pmid", "")
                title = article.get("title", "Unknown")[:100]
                doi = article.get("doi", "").lower()

                # Check if duplicate
                if skip_duplicates:
                    if pmid in existing_identifiers.get("existing_pmids", set()):
                        result.add_item(
                            ImportedItem(
                                pmid=pmid,
                                title=title,
                                action=ImportAction.SKIPPED,
                                reason=f"PMID already exists (key: {pmid_to_key.get(pmid, 'unknown')})",
                            )
                        )
                        continue

                    if doi and doi in existing_identifiers.get("existing_dois", set()):
                        result.add_item(
                            ImportedItem(
                                pmid=pmid,
                                title=title,
                                action=ImportAction.SKIPPED,
                                reason=f"DOI already exists (key: {doi_to_key.get(doi, 'unknown')})",
                            )
                        )
                        continue

                # Map to Zotero schema (complete metadata!)
                try:
                    # Pass validated collection_key to mapper
                    collection_keys = [validated_collection_key] if validated_collection_key else None
                    zotero_item = map_pubmed_to_zotero(
                        article,
                        extra_tags=tags,
                        collection_keys=collection_keys,
                    )
                    items_to_save.append((pmid, title, zotero_item))
                except Exception as e:
                    logger.error(f"Failed to map article {pmid}: {e}")
                    result.add_item(
                        ImportedItem(
                            pmid=pmid,
                            title=title,
                            action=ImportAction.FAILED,
                            error=f"Mapping error: {e}",
                        )
                    )

            # 5. Batch save to Zotero
            if items_to_save:
                try:
                    # Extract just the items for saving
                    zotero_items = [item[2] for item in items_to_save]

                    await zotero_client.batch_save_items(
                        items=zotero_items,
                        uri="http://mcp-bridge.local/batch-import-from-pubmed",
                        title="PubMed Batch Import",
                    )

                    # Process save results
                    # Note: Zotero Connector API doesn't return individual keys easily
                    # So we mark all as added for now
                    for pmid, title, _ in items_to_save:
                        result.add_item(
                            ImportedItem(
                                pmid=pmid,
                                title=title,
                                action=ImportAction.ADDED,
                                zotero_key=None,  # Key not available from Connector API
                            )
                        )

                    logger.info(f"Saved {len(items_to_save)} items to Zotero")

                except Exception as e:
                    logger.error(f"Failed to save to Zotero: {e}")
                    # Mark all pending items as failed
                    for pmid, title, _ in items_to_save:
                        result.add_item(
                            ImportedItem(
                                pmid=pmid,
                                title=title,
                                action=ImportAction.FAILED,
                                error=f"Save error: {e}",
                            )
                        )

            # 6. Record collection info in result
            if validated_collection_key:
                result.collection_key = validated_collection_key
                logger.info(f"Items added to collection: {validated_collection_key}")

            # 7. Finalize result
            result.elapsed_time = time.time() - start_time

            final_result = result.to_dict()

            # åŠ å…¥ collection_info è®“ä½¿ç”¨è€…ç¢ºèª
            if collection_info:
                final_result["collection_info"] = collection_info
                logger.info(f"Collection info: {collection_info}")

            return final_result

        except Exception as e:
            logger.error(f"Batch import failed: {e}")
            result.success = False
            result.elapsed_time = time.time() - start_time
            return {
                **result.to_dict(),
                "error": str(e),
            }

    logger.info("Batch import tools registered (batch_import_from_pubmed)")


def _parse_pmids(pmids_input: str) -> list[str]:
    """
    Parse PMID input string to list of PMIDs.

    Args:
        pmids_input: Comma-separated PMIDs or special values like "last"

    Returns:
        List of PMID strings
    """
    if not pmids_input:
        return []

    # Handle "last" keyword (would need session state)
    if pmids_input.strip().lower() == "last":
        # TODO: Implement session-based last search results
        logger.warning("'last' keyword not yet implemented")
        return []

    # Parse comma-separated PMIDs
    pmids = []
    for pmid in pmids_input.split(","):
        pmid = pmid.strip()
        if pmid and pmid.isdigit():
            pmids.append(pmid)
        elif pmid:
            logger.warning(f"Invalid PMID skipped: {pmid}")

    return pmids
