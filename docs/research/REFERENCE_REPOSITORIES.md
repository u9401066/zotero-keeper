# Reference Repositories for Literature Search Tools

> ğŸ“š æœ¬æ–‡æª”è©³ç´°è¨˜éŒ„ 5 å€‹é‡è¦çš„å­¸è¡“æ–‡ç»æœå°‹é–‹æºå°ˆæ¡ˆï¼Œä½œç‚º pubmed-search-mcp çš„å­¸ç¿’åƒè€ƒã€‚

---

## ç¸½è¦½

| Repo | Stars | èªè¨€ | æˆæ¬Š | æœ€å¾Œæ›´æ–° |
|------|-------|------|------|----------|
| [scholarly](https://github.com/scholarly-python-package/scholarly) | 1.8k | Python | Unlicense | æ´»èº |
| [habanero](https://github.com/sckott/habanero) | 238 | Python | MIT | æ´»èº |
| [pyalex](https://github.com/J535D165/pyalex) | 325 | Python | MIT | æ´»èº |
| [metapub](https://github.com/metapub/metapub) | 140 | Python | Apache-2.0 | æ´»èº |
| [bioservices](https://github.com/cokelaer/bioservices) | 325 | Python | GPL-3.0 | æ´»èº |

---

## 1. scholarly (Google Scholar çˆ¬èŸ²)

### åŸºæœ¬è³‡è¨Š

- **GitHub**: https://github.com/scholarly-python-package/scholarly
- **Stars**: 1,800+
- **æˆæ¬Š**: Unlicense (å…¬å…±é ˜åŸŸ)
- **å®‰è£**: `pip install scholarly`

### æ ¸å¿ƒåŠŸèƒ½

```python
from scholarly import scholarly

# æœå°‹è«–æ–‡
search_query = scholarly.search_pubs('deep learning')
paper = next(search_query)

# å–å¾—å®Œæ•´è³‡æ–™ï¼ˆå»¶é²è¼‰å…¥ï¼‰
paper_filled = scholarly.fill(paper)

# æœå°‹ä½œè€…
author = scholarly.search_author_id('EmD_lTEAAAAJ')  # Geoffrey Hinton
author_filled = scholarly.fill(author)

# å–å¾—å¼•ç”¨æ­¤è«–æ–‡çš„æ–‡ç« 
citations = scholarly.citedby(paper)
```

### é—œéµæ¶æ§‹å­¸ç¿’

#### 1.1 ProxyGenerator - ä»£ç†æ± ç®¡ç†

```python
from scholarly import ProxyGenerator

pg = ProxyGenerator()

# æ–¹å¼ 1: ScraperAPI (å•†æ¥­æœå‹™)
pg.ScraperAPI('YOUR_API_KEY')

# æ–¹å¼ 2: Tor ç¶²è·¯
pg.Tor_Internal()

# æ–¹å¼ 3: å…è²»ä»£ç†æ± 
pg.FreeProxies()

# è¨­å®šä»£ç†
scholarly.use_proxy(pg)
```

**å­¸ç¿’é‡é»**ï¼š
- æŠ½è±¡ä»£ç†ä»‹é¢ï¼Œæ”¯æ´å¤šç¨®ä»£ç†ä¾†æº
- è‡ªå‹•è¼ªæ›å’Œå¤±æ•—é‡è©¦
- é¿å… Google çš„ CAPTCHA å°é–

#### 1.2 fill() å»¶é²è¼‰å…¥æ¨¡å¼

```python
# åˆå§‹æœå°‹è¿”å›åŸºæœ¬è³‡è¨Š
paper = next(scholarly.search_pubs('attention is all you need'))
# paper.bib åªæœ‰æ¨™é¡Œã€ä½œè€…ç­‰åŸºæœ¬è³‡è¨Š

# fill() å–å¾—å®Œæ•´è³‡è¨Š
paper = scholarly.fill(paper)
# ç¾åœ¨æœ‰å®Œæ•´æ‘˜è¦ã€å¼•ç”¨æ•¸ã€PDF é€£çµç­‰
```

**å­¸ç¿’é‡é»**ï¼š
- æ¸›å°‘ä¸å¿…è¦çš„ API è«‹æ±‚
- ç”¨æˆ¶åªåœ¨éœ€è¦æ™‚å–å¾—å®Œæ•´è³‡æ–™
- å¯æ‡‰ç”¨æ–¼æˆ‘å€‘çš„ `fetch_article_details`

#### 1.3 å¼•ç”¨ç¶²è·¯éæ­·

```python
# å–å¾—å¼•ç”¨æ­¤è«–æ–‡çš„æ–‡ç« 
for citing_paper in scholarly.citedby(paper):
    print(citing_paper['bib']['title'])

# å–å¾—ä½œè€…çš„æ‰€æœ‰æ–‡ç« 
for pub in author['publications']:
    filled_pub = scholarly.fill(pub)
```

### æ•´åˆå»ºè­°

| åŠŸèƒ½ | å„ªå…ˆç´š | å¯¦ä½œæ–¹å¼ |
|------|--------|----------|
| Google Scholar å¼•ç”¨æ•¸ | ä¸­ | ä½œç‚º iCite è£œå…… |
| ä½œè€… h-index | ä½ | æ–°å¢ `get_author_metrics` |
| ä»£ç†è¼ªæ›æ©Ÿåˆ¶ | é«˜ | ç”¨æ–¼é«˜é »æœå°‹å ´æ™¯ |

---

## 2. habanero (CrossRef API)

### åŸºæœ¬è³‡è¨Š

- **GitHub**: https://github.com/sckott/habanero
- **Stars**: 238
- **æˆæ¬Š**: MIT
- **å®‰è£**: `pip install habanero`

### æ ¸å¿ƒåŠŸèƒ½

```python
from habanero import Crossref, counts, cn

cr = Crossref()

# æœå°‹ä½œå“
result = cr.works(query="machine learning")

# é€é DOI å–å¾—å…ƒæ•¸æ“š
work = cr.works(ids="10.1038/nature12373")

# å–å¾—å¼•ç”¨æ•¸
citation_count = counts.citation_count(doi="10.1038/nature12373")
```

### é—œéµæ¶æ§‹å­¸ç¿’

#### 2.1 Content Negotiation (cn æ¨¡çµ„)

```python
from habanero import cn

doi = "10.1126/science.169.3946.635"

# å–å¾— BibTeX æ ¼å¼
bibtex = cn.content_negotiation(ids=doi, format="bibtex")

# å–å¾— RIS æ ¼å¼
ris = cn.content_negotiation(ids=doi, format="ris")

# å–å¾— Citeproc JSON
citeproc = cn.content_negotiation(ids=doi, format="citeproc-json")

# æ”¯æ´çš„æ ¼å¼
# rdf-xml, turtle, citeproc-json, citeproc-json-ish
# text, ris, bibtex, crossref-xml, datacite-xml
```

**å­¸ç¿’é‡é»**ï¼š
- å–®ä¸€ DOI å¯è¼¸å‡ºå¤šç¨®å¼•ç”¨æ ¼å¼
- æ¨™æº– HTTP Content Negotiation
- å¯å¢å¼·æˆ‘å€‘çš„ `prepare_export` åŠŸèƒ½

#### 2.2 Polite Pool æ©Ÿåˆ¶

```python
from habanero import Crossref

# è¨­å®š email ä»¥ç²å¾—æ›´é«˜é€Ÿç‡é™åˆ¶
cr = Crossref(mailto="your@email.com")

# CrossRef æœƒå°‡ä½ åŠ å…¥ "polite pool"
# é€Ÿç‡é™åˆ¶å¾ 50 req/s æå‡åˆ°æ›´é«˜
```

**å­¸ç¿’é‡é»**ï¼š
- ç°¡å–®è¨­å®šå³å¯ç²å¾—æ›´å¥½çš„ API é«”é©—
- æˆ‘å€‘çš„ CrossRef client æ‡‰æ¡ç”¨åŒæ¨£åšæ³•

#### 2.3 Reference é€£çµè¿½è¹¤

```python
# å–å¾—è«–æ–‡çš„åƒè€ƒæ–‡ç» DOI
work = cr.works(ids="10.1038/nature12373")
references = work['message'].get('reference', [])

for ref in references:
    if 'DOI' in ref:
        print(f"Reference DOI: {ref['DOI']}")
```

### æ•´åˆå»ºè­°

| åŠŸèƒ½ | å„ªå…ˆç´š | å¯¦ä½œæ–¹å¼ |
|------|--------|----------|
| Content Negotiation | é«˜ | å¢å¼· `prepare_export` |
| Polite Pool | é«˜ | æ›´æ–° `sources/crossref.py` |
| Reference DOI æå– | ä¸­ | å¢å¼· `get_article_references` |

---

## 3. pyalex (OpenAlex API)

### åŸºæœ¬è³‡è¨Š

- **GitHub**: https://github.com/J535D165/pyalex
- **Stars**: 325
- **æˆæ¬Š**: MIT
- **å®‰è£**: `pip install pyalex`

### æ ¸å¿ƒåŠŸèƒ½

```python
import pyalex
from pyalex import Works, Authors, Sources, Institutions

# è¨­å®š email (polite pool)
pyalex.config.email = "your@email.com"

# Pipe æ“ä½œé¢¨æ ¼æœå°‹
works = Works().filter(publication_year=2023).filter(open_access={"is_oa": True}).get()

# é€é ID å–å¾—
work = Works()["W2741809807"]
author = Authors()["A5023888391"]

# N-grams æ”¯æ´
ngrams = Works()["W2023271753"].ngrams()
```

### é—œéµæ¶æ§‹å­¸ç¿’

#### 3.1 Pipe æ“ä½œéˆ

```python
from pyalex import Works

# éˆå¼ API è¨­è¨ˆ
results = (
    Works()
    .filter(publication_year=">2020")
    .filter(concepts={"id": "C41008148"})  # Computer Science
    .filter(is_oa=True)
    .sort(cited_by_count="desc")
    .get()
)

# å–å¾—ç‰¹å®šé 
page_3 = Works().filter(publication_year=2023).get(page=3, per_page=50)
```

**å­¸ç¿’é‡é»**ï¼š
- æµæš¢ API è¨­è¨ˆæå‡é–‹ç™¼é«”é©—
- å¯æ‡‰ç”¨æ–¼æˆ‘å€‘çš„æœå°‹å·¥å…·éˆ

#### 3.2 Abstract åå‘ç´¢å¼•è½‰æ›

OpenAlex ä½¿ç”¨åå‘ç´¢å¼•å„²å­˜æ‘˜è¦ï¼š

```python
# OpenAlex åŸå§‹æ ¼å¼
abstract_inverted_index = {
    "This": [0],
    "is": [1, 4],
    "a": [2],
    "test": [3],
    "paper": [5]
}

# pyalex è‡ªå‹•è½‰æ›
from pyalex import Works

work = Works()["W2741809807"]
plain_abstract = work['abstract']  # è‡ªå‹•è½‰ç‚ºç´”æ–‡å­—
```

**å­¸ç¿’é‡é»**ï¼š
- è³‡æ–™æ ¼å¼è½‰æ›çš„å°è£
- éš±è—è¤‡é›œæ€§ï¼Œæä¾›ç°¡æ½”ä»‹é¢

#### 3.3 Cursor-based Pagination

```python
from pyalex import Works

# ä½¿ç”¨ cursor å–å¾—æ‰€æœ‰çµæœ
all_works = []
for page in Works().filter(publication_year=2023).paginate(per_page=200):
    all_works.extend(page)
    if len(all_works) >= 10000:
        break
```

**å­¸ç¿’é‡é»**ï¼š
- å¤§é‡è³‡æ–™çš„é«˜æ•ˆåˆ†é 
- Iterator æ¨¡å¼è™•ç†æµå¼è³‡æ–™

### æ•´åˆå»ºè­°

| åŠŸèƒ½ | å„ªå…ˆç´š | å¯¦ä½œæ–¹å¼ |
|------|--------|----------|
| N-grams è¶¨å‹¢åˆ†æ | ä¸­ | æ–°å¢ `analyze_topic_trends` |
| Concepts æ¢ç´¢ | ä¸­ | æ–°å¢ `explore_concepts` |
| æµæš¢ API | ä½ | é‡æ§‹ç¾æœ‰å®¢æˆ¶ç«¯ |

---

## 4. metapub (NCBI å·¥å…·åŒ…) â­â­ é«˜åº¦ç›¸é—œ

### åŸºæœ¬è³‡è¨Š

- **GitHub**: https://github.com/metapub/metapub
- **Stars**: 140
- **æˆæ¬Š**: Apache-2.0
- **å®‰è£**: `pip install metapub`

### æ ¸å¿ƒåŠŸèƒ½

```python
from metapub import PubMedFetcher, FindIt, CrossRefFetcher

# PubMed æœå°‹
fetch = PubMedFetcher()
pmids = fetch.pmids_for_query("cancer treatment 2023")
article = fetch.article_by_pmid('12345678')

# PDF ç™¼ç¾
url = FindIt('12345678').url  # è‡ªå‹•æ‰¾åˆ° PDF é€£çµ

# CrossRef æ•´åˆ
cr = CrossRefFetcher()
article = cr.article_by_doi('10.1038/nature12373')
```

### é—œéµæ¶æ§‹å­¸ç¿’

#### 4.1 FindIt - PDF ç™¼ç¾å¼•æ“ â­â­

**é€™æ˜¯ metapub æœ€æœ‰åƒ¹å€¼çš„åŠŸèƒ½**ï¼š

```python
from metapub import FindIt

# è‡ªå‹•ç™¼ç¾ PDF é€£çµ
src = FindIt('23132851')
print(src.url)  # https://www.nature.com/articles/nature12373.pdf
print(src.reason)  # 'DOI lookup' æˆ– 'PubMed Central' ç­‰

# æ”¯æ´ 68+ å‡ºç‰ˆå•†çš„ URL è¦å‰‡
# - Elsevier (ScienceDirect)
# - Springer Nature
# - Wiley
# - Oxford University Press
# - Taylor & Francis
# - SAGE
# - American Chemical Society
# - ç­‰ç­‰...
```

**å­¸ç¿’é‡é»**ï¼š
- ç¶­è­·å‡ºç‰ˆå•† URL è¦å‰‡è³‡æ–™åº«
- å¤šç­–ç•¥å˜—è©¦ï¼ˆDOI lookup â†’ PMC â†’ Publisher siteï¼‰
- å¯é¡¯è‘—å¢å¼·æˆ‘å€‘çš„å…¨æ–‡å–å¾—èƒ½åŠ›

#### 4.2 UrlReverse - URL è­˜åˆ¥

```python
from metapub import UrlReverse

# å¾ URL è­˜åˆ¥è«–æ–‡
ur = UrlReverse("https://www.nature.com/articles/nature12373")
print(ur.doi)   # 10.1038/nature12373
print(ur.pmid)  # 23132851
```

**å­¸ç¿’é‡é»**ï¼š
- åå‘å·¥ç¨‹ URL æå–è­˜åˆ¥ç¢¼
- å¯å¢åŠ  "å¾ URL æœå°‹" åŠŸèƒ½

#### 4.3 çµ±ä¸€ Article ä»‹é¢

```python
from metapub import PubMedArticle, CrossRefArticle

# å…©ç¨®ä¾†æºä½¿ç”¨ç›¸åŒä»‹é¢
pm_article = PubMedFetcher().article_by_pmid('12345678')
cr_article = CrossRefFetcher().article_by_doi('10.1038/xxx')

# çµ±ä¸€å±¬æ€§
print(pm_article.title)
print(pm_article.authors)
print(pm_article.abstract)
print(pm_article.citation)  # æ ¼å¼åŒ–å¼•ç”¨
```

**å­¸ç¿’é‡é»**ï¼š
- é¡ä¼¼æˆ‘å€‘çš„ `UnifiedArticle` è¨­è¨ˆ
- å¯åƒè€ƒå…¶å±¬æ€§å®šç¾©

### æ•´åˆå»ºè­°

| åŠŸèƒ½ | å„ªå…ˆç´š | å¯¦ä½œæ–¹å¼ |
|------|--------|----------|
| FindIt PDF ç™¼ç¾ | **æ¥µé«˜** | Fork æˆ–æ•´åˆå…¶é‚è¼¯ |
| UrlReverse | ä¸­ | æ–°å¢ `identify_url` å·¥å…· |
| MedGen/ClinVar | ä½ | èˆ‡ `search_clinvar` æ•´åˆ |

### FindIt æ•´åˆè¨ˆç•«

```python
# å»ºè­°çš„æ•´åˆæ–¹å¼
# external/pubmed-search-mcp/src/pubmed_search/fulltext/findit.py

from metapub import FindIt

class FulltextFinder:
    """æ•´åˆ metapub FindIt çš„å…¨æ–‡ç™¼ç¾å™¨"""

    def find_pdf_url(self, pmid: str) -> dict:
        """æ‰¾å‡ºè«–æ–‡çš„ PDF é€£çµ"""
        try:
            src = FindIt(pmid)
            return {
                "pmid": pmid,
                "pdf_url": src.url,
                "source": src.reason,
                "backup_urls": src.backup_url_list,
            }
        except Exception as e:
            return {"pmid": pmid, "error": str(e)}
```

---

## 5. bioservices (å¤šæœå‹™æ¡†æ¶)

### åŸºæœ¬è³‡è¨Š

- **GitHub**: https://github.com/cokelaer/bioservices
- **Stars**: 325
- **æˆæ¬Š**: GPL-3.0
- **å®‰è£**: `pip install bioservices`

### æ ¸å¿ƒåŠŸèƒ½

```python
from bioservices import UniProt, KEGG, ChEMBL, PubChem

# UniProt è›‹ç™½è³ªè³‡æ–™åº«
u = UniProt()
result = u.search("BRCA1", limit=10)

# KEGG ä»£è¬é€”å¾‘
k = KEGG()
pathway = k.get("hsa:7157")  # p53

# ChEMBL è—¥ç‰©è³‡æ–™åº«
c = ChEMBL()
compounds = c.get_molecule_by_chemblId("CHEMBL25")

# PubChem åŒ–åˆç‰©
p = PubChem()
compound = p.get_compound_by_name("aspirin")
```

### é—œéµæ¶æ§‹å­¸ç¿’

#### 5.1 çµ±ä¸€æœå‹™æŠ½è±¡å±¤

```python
from bioservices import REST, WSDL

class MyService(REST):
    """è‡ªè¨‚æœå‹™çš„åŸºç¤é¡åˆ¥"""

    _url = "https://api.example.com"

    def __init__(self, verbose=False):
        super().__init__(name="MyService", url=self._url, verbose=verbose)

    def search(self, query):
        return self.http_get(f"search?q={query}")
```

**å­¸ç¿’é‡é»**ï¼š
- REST/WSDL åŸºç¤é¡åˆ¥æä¾›çµ±ä¸€ä»‹é¢
- è‡ªå‹•è™•ç†é‡è©¦ã€å¿«å–ã€éŒ¯èª¤
- å¯åƒè€ƒè¨­è¨ˆæˆ‘å€‘çš„ `sources/base.py`

#### 5.2 å‘½ä»¤åˆ—å·¥å…·

```bash
# å…§å»º CLI
bioservices download-accession P12345 --db uniprot
bioservices search "BRCA1" --db uniprot --limit 10
```

**å­¸ç¿’é‡é»**ï¼š
- æœå‹™å±¤å¯ç›´æ¥æš´éœ²ç‚º CLI
- æ–¹ä¾¿èª¿è©¦å’Œç¨ç«‹ä½¿ç”¨

#### 5.3 æ”¯æ´çš„æœå‹™åˆ—è¡¨

bioservices åŒ…è£äº† 40+ æœå‹™ï¼Œå…¶ä¸­èˆ‡æˆ‘å€‘ç›¸é—œçš„ï¼š

| æœå‹™ | åŠŸèƒ½ | ç›¸é—œæ€§ |
|------|------|--------|
| EUtils | NCBI E-utilities | é«˜ |
| PubChem | åŒ–åˆç‰©è³‡æ–™åº« | ä¸­ |
| ChEMBL | è—¥ç‰©è³‡æ–™åº« | ä¸­ |
| UniProt | è›‹ç™½è³ªè³‡æ–™åº« | ä½ |
| KEGG | ä»£è¬é€”å¾‘ | ä½ |

### æ•´åˆå»ºè­°

| åŠŸèƒ½ | å„ªå…ˆç´š | å¯¦ä½œæ–¹å¼ |
|------|--------|----------|
| æœå‹™æŠ½è±¡å±¤è¨­è¨ˆ | ä¸­ | é‡æ§‹ `sources/` æ¶æ§‹ |
| CLI å·¥å…· | ä½ | æ–°å¢ `pubmed-mcp-cli` |
| PubChem æ·±åº¦æ•´åˆ | ä¸­ | å¢å¼· `get_compound_details` |

---

## é—œæ–¼è«–æ–‡åœ–ç‰‡ API ğŸ“·

### å•é¡Œ

> PubMed å®˜æ–¹ API æ˜¯å¦æä¾›è«–æ–‡å…§åœ–ç‰‡é€£çµï¼Ÿ

### ç­”æ¡ˆ

**PubMed E-utilities æ˜¯ç´”æ–‡å­— APIï¼Œä¸ç›´æ¥æä¾›åœ–ç‰‡é€£çµ**

### æ›¿ä»£æ–¹æ¡ˆ

#### 1. PMC Open Access (æœ€å¯é )

```xml
<!-- PMC å…¨æ–‡ XML ä¸­çš„åœ–ç‰‡å…ƒç´  -->
<fig id="fig1">
  <label>Figure 1</label>
  <caption>
    <p>Experimental design...</p>
  </caption>
  <graphic xlink:href="PMC7096777_fig1.jpg"/>
</fig>
```

**å–å¾—æ–¹å¼**ï¼š
1. ä½¿ç”¨ `get_fulltext_xml(pmcid)` å–å¾— JATS XML
2. è§£æ `<fig>` å…ƒç´ 
3. çµ„åˆåœ–ç‰‡ URLï¼š
   ```
   https://www.ncbi.nlm.nih.gov/pmc/articles/{PMCID}/bin/{filename}
   ```

#### 2. Europe PMC Text-Mining API

```python
# å–å¾— FIGURE é¡å‹çš„æ¨™è¨»
from pubmed_search.mcp.tools.fulltext import get_text_mined_terms

# semantic_type="FIGURE" å¯å–å¾—åœ–ç‰‡ç›¸é—œæ¨™è¨»
figures = get_text_mined_terms(pmid="12345678", semantic_type="FIGURE")
```

#### 3. bioRxiv/medRxiv

é å°æœ¬æœå‹™å™¨é€šå¸¸ç›´æ¥åœ¨ HTML ä¸­æš´éœ²åœ–ç‰‡ URLï¼š

```
https://www.biorxiv.org/content/10.1101/2024.01.01.001.full/figure/F1.large.jpg
```

### å¯¦ä½œå»ºè­°

```python
# å»ºè­°æ–°å¢å·¥å…·: get_article_figures

async def get_article_figures(pmcid: str) -> dict:
    """
    å¾ PMC å…¨æ–‡å–å¾—è«–æ–‡åœ–ç‰‡åˆ—è¡¨ã€‚

    Args:
        pmcid: PMC ID (å¦‚ "PMC7096777")

    Returns:
        {
            "figures": [
                {
                    "id": "fig1",
                    "label": "Figure 1",
                    "caption": "Experimental design...",
                    "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7096777/bin/fig1.jpg"
                }
            ]
        }
    """
    xml = await get_fulltext_xml(pmcid)
    # è§£æ XML æå– <fig> å…ƒç´ ...
```

---

## å­¸ç¿’è¡Œå‹•è¨ˆç•«

### ç«‹å³ (v1.12.0)

1. **æ•´åˆ habanero Content Negotiation**
   - æ›´æ–° `sources/crossref.py` åŠ å…¥ `cn` åŠŸèƒ½
   - å¢å¼· `prepare_export` æ”¯æ´æ›´å¤šæ ¼å¼

2. **æ¡ç”¨ metapub FindIt é‚è¼¯**
   - ç ”ç©¶ FindIt çš„å‡ºç‰ˆå•† URL è¦å‰‡
   - æ–°å¢ `find_fulltext_url` å·¥å…·

3. **å¯¦ä½œ PMC åœ–ç‰‡æå–**
   - è§£æ JATS XML ä¸­çš„ `<fig>` å…ƒç´ 
   - æ–°å¢ `get_article_figures` å·¥å…·

### ä¸­æœŸ (v1.13.0)

4. **pyalex N-grams è¶¨å‹¢åˆ†æ**
   - æ–°å¢ `analyze_topic_trends` å·¥å…·

5. **bioservices æ¡†æ¶åƒè€ƒ**
   - é‡æ§‹ `sources/` æ¨¡çµ„
   - å»ºç«‹çµ±ä¸€æœå‹™åŸºç¤é¡åˆ¥

### é•·æœŸ

6. **scholarly Google Scholar æ•´åˆ**
   - è©•ä¼°æ³•å¾‹é¢¨éšªå’Œç©©å®šæ€§
   - ä½œç‚ºè£œå……å¼•ç”¨ä¾†æº

---

## 6. Web of Science Starter API (Clarivate å®˜æ–¹)

### åŸºæœ¬è³‡è¨Š

- **Developer Portal**: https://developer.clarivate.com/apis/wos-starter
- **GitHub**: https://github.com/clarivate/wosstarter_python_client
- **Stars**: 29 (å®˜æ–¹ç¶­è­·)
- **æˆæ¬Š**: OpenAPI ç”Ÿæˆ
- **å®‰è£**: `pip install git+https://github.com/clarivate/wosstarter_python_client.git`

### æ ¸å¿ƒåŠŸèƒ½

```python
import clarivate.wos_starter.client as wos

# é…ç½® API Key
configuration = wos.Configuration(
    host="https://api.clarivate.com/apis/wos-starter/v1"
)
configuration.api_key['ClarivateApiKeyAuth'] = 'YOUR_API_KEY'

with wos.ApiClient(configuration) as api_client:
    api = wos.DocumentsApi(api_client)

    # æœå°‹æ–‡ç»
    result = api.documents_get(
        q='TS=machine learning AND PY=2024',
        db='WOS',
        limit=10,
        sort_field='TC+D'  # æŒ‰å¼•ç”¨æ•¸é™åº
    )

    # å–å¾—å–®ç¯‡æ–‡ç»
    doc = api.documents_uid_get(uid='WOS:000123456789')
```

### é—œéµç‰¹è‰²

#### 6.1 Times Cited æ•¸æ“š

**é€™æ˜¯ WoS æœ€æœ‰åƒ¹å€¼çš„æ•¸æ“š**ï¼š

```python
# è¿”å›çš„æ–‡ç»åŒ…å« times_cited
for doc in result.data:
    print(f"{doc.title}: {doc.times_cited} citations")
```

**å„ªå‹¢**ï¼š
- å®˜æ–¹å¼•ç”¨æ•¸æ“šï¼ˆæ¯” Google Scholar æ›´æ¬Šå¨ï¼‰
- åŒ…å« JCR (Journal Citation Reports) é€£çµ
- æ”¯æ´ Web of Science Core Collection å®Œæ•´æ¬„ä½

#### 6.2 é«˜ç´šæœå°‹èªæ³•

```python
# æ”¯æ´çš„æœå°‹æ¬„ä½
queries = [
    'TI=deep learning',           # æ¨™é¡Œ
    'AU=Smith, John',             # ä½œè€…
    'TS=CRISPR',                  # ä¸»é¡Œ (æ¨™é¡Œ+æ‘˜è¦+é—œéµå­—)
    'DO=10.1038/nature12373',     # DOI
    'PMID=23132851',              # PubMed ID
    'PY=2020-2024',               # å¹´ä»½ç¯„åœ
    'OG=Harvard University',       # æ©Ÿæ§‹
]
```

#### 6.3 å¤šè³‡æ–™åº«æ”¯æ´

```python
# æ”¯æ´çš„è³‡æ–™åº«
databases = [
    'WOS',      # Web of Science Core Collection
    'MEDLINE',  # MEDLINE
    'BIOSIS',   # BIOSIS Previews
    'DRCI',     # Data Citation Index
    'PPRN',     # Preprint Citation Index
    'WOK',      # All databases
]
```

### æ–¹æ¡ˆå±¤ç´š

| æ–¹æ¡ˆ | è«‹æ±‚é™åˆ¶ | Times Cited | é©ç”¨å°è±¡ |
|------|----------|-------------|----------|
| Free Trial | 50/day | âŒ | è©•ä¼°ç”¨é€” |
| Institutional Member | 5,000/day | âœ… | è¨‚é–±æ©Ÿæ§‹æˆå“¡ |
| Integration | 20,000/day | âœ… | æ©Ÿæ§‹ç³»çµ±æ•´åˆ |

### æ•´åˆå»ºè­°

| åŠŸèƒ½ | å„ªå…ˆç´š | å¯¦ä½œæ–¹å¼ |
|------|--------|----------|
| Times Cited è£œå…… | ä¸­ | èˆ‡ iCite ä¸¦è¡Œæä¾› |
| JCR é€£çµ | ä½ | æœŸåˆŠå½±éŸ¿å› å­åƒè€ƒ |
| WoS è­˜åˆ¥ç¢¼ | ä½ | UID äº¤å‰å¼•ç”¨ |

**æ³¨æ„**ï¼šéœ€è¦ API Keyï¼Œä¸” Times Cited éœ€è¦æ©Ÿæ§‹è¨‚é–±ã€‚

---

## æŒçºŒå­¸ç¿’å»ºè­°

### æ¯å­£åº¦ Review æ¸…å–®

1. **æª¢æŸ¥ Release Notes**
   - å„ repo çš„ GitHub Releases
   - API æ›´æ–°å’Œæ–°ç«¯é»

2. **è¿½è¹¤æ–°åŠŸèƒ½**
   - scholarly: æ–°çš„åçˆ¬èŸ²ç­–ç•¥
   - habanero: CrossRef API æ›´æ–°
   - pyalex: OpenAlex æ–° Concepts
   - metapub: FindIt æ–°å‡ºç‰ˆå•†è¦å‰‡

3. **ç¤¾ç¾¤å‹•æ…‹**
   - GitHub Issues å’Œ Discussions
   - Stack Overflow ç›¸é—œå•é¡Œ

### å­¸ç¿’è³‡æº

| è³‡æº | é€£çµ |
|------|------|
| PubMed API | https://www.ncbi.nlm.nih.gov/books/NBK25501/ |
| CrossRef API | https://api.crossref.org/swagger-ui/index.html |
| OpenAlex API | https://docs.openalex.org/ |
| Europe PMC API | https://europepmc.org/RestfulWebService |
| Unpaywall API | https://unpaywall.org/products/api |
| Web of Science API | https://developer.clarivate.com/apis/wos-starter |

---

*æœ€å¾Œæ›´æ–°: 2025 å¹´ 1 æœˆ*
